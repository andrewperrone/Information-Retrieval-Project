"""
Emotion Deep Dive Analyzer

This diagnostic tool allows for granular inspection of a specific book's emotional fingerprint.
It calculates and displays the raw counts, density, and Z-scores for all 8 core emotions,
helping to verify why a book was ranked in a specific way by the IR system.

Action:
It loads the pre-computed emotion data and statistics, searches for a book by title,
and prints a detailed statistical breakdown of its emotional content.

Connection:
This is a standalone analysis tool. It consumes data from 'emotion_results.pkl',
'search_index.pkl', and 'emotion_stats.pkl', but does not generate new data for the pipeline.

Inputs:
- 'emotion_results.pkl' (Generated by book_level_emotion_analyzer.py)
- 'search_index.pkl' (Generated by indexer.py, used for document lengths)
- 'emotion_stats.pkl' (Generated by z_score_generator.py, used for Z-score math)
- User input: Book title fragment (e.g., "Frankenstein")

Outputs:
- Console printout of the book's emotional profile
- Z-Scores for all 8 emotions, sorted by intensity
- Visual indicators ("<<") for statistically significant outliers

Process:
1. Loads all necessary pickle files into memory.
2. Accepts a user input string to search for a specific book ID.
3. Retrieves the raw emotion vector and document length for that book.
4. For each of the 8 emotions:
   - Calculates Density (Count / Length)
   - Calculates Z-Score ((Density - Corpus_Mean) / Corpus_Std_Dev)
5. Sorts emotions from highest Z-Score to lowest.
6. Prints a formatted table to the console.
"""

import pickle
import numpy as np
import os

# --- Configuration ---
EMOTION_FILE = "emotion_results.pkl"
INDEX_FILE = "search_index.pkl"
STATS_FILE = "emotion_stats.pkl"
# ---------------------

def analyze_book(target_title_fragment):
    print(f"Loading data...")
    
    if not os.path.exists(EMOTION_FILE) or not os.path.exists(STATS_FILE):
        print("Error: Missing .pkl data files.")
        return

    # Load Data
    with open(EMOTION_FILE, 'rb') as f:
        raw_data = pickle.load(f)
        emotion_data = {item[0]: item[1] for item in raw_data}

    with open(INDEX_FILE, 'rb') as f:
        index_data = pickle.load(f)
        doc_lengths = index_data.get('doc_lengths', {})

    with open(STATS_FILE, 'rb') as f:
        stats = pickle.load(f)

    # Find the book ID
    found_id = None
    target_clean = target_title_fragment.lower().replace(" ", "")
    for doc_id in emotion_data.keys():
        if target_clean in doc_id.lower().replace("_", "").replace("-", ""):
            found_id = doc_id
            break
    
    if not found_id:
        print(f"Book containing '{target_title_fragment}' not found.")
        return

    # Calculate Metrics
    print(f"\n{'='*60}")
    print(f"DEEP DIVE ANALYSIS: {found_id}")
    print(f"{'='*60}")
    
    length = doc_lengths.get(found_id, 1)
    vector = emotion_data[found_id]
    
    # Store results for sorting
    results = []
    
    print(f"{'Emotion':<15} | {'Raw Count':<10} | {'Density':<10} | {'Z-Score (The Vibe)'}")
    print("-" * 65)
    
    for emo in ['joy', 'sadness', 'anger', 'fear', 'trust', 'disgust', 'anticipation', 'surprise']:
        count = vector.get(emo, 0)
        density = count / length
        
        # Calculate Z-Score
        mean = stats[emo]['mean']
        std = stats[emo]['std']
        z_score = (density - mean) / std
        
        results.append((emo, count, density, z_score))

    # Sort by Z-Score (High to Low)
    results.sort(key=lambda x: x[3], reverse=True)
    
    for emo, count, dens, z in results:
        # Highlight strong signals
        mark = "<<" if z > 1.0 else "" 
        print(f"{emo:<15} | {count:<10} | {dens:.5f}    | {z:+.2f}  {mark}")

if __name__ == "__main__":
    while True:
        query = input("\nEnter book title to analyze (or 'exit'): ").strip()
        if query.lower() == 'exit':
            break
        analyze_book(query)